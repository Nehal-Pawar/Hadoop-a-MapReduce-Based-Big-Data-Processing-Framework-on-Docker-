
#!/bin/bash

# test the hadoop cluster by running wordcount
rm -r input
hadoop fs -rm -r input
hadoop fs -rm -r output

# create input files

mkdir input

#echo "Hello Docker" >input/file2.txt
#echo "Hello Hadoop" >input/file1.txt

#cp text.txt input/text.txt

cp uber input/uber

# create input directory on HDFS
hadoop fs -mkdir -p input

# put input files to HDFS
hdfs dfs -put ./input/* input

# run wordcount
#hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.7.2-sources.jar org.apache.hadoop.examples.WordCount input output

#hadoop jar wc.jar WordCount input output

hadoop jar wc.jar Uber1 input output

# print the input files
echo -e "\ninput file1.txt:"
hdfs dfs -cat input/file1.txt

echo -e "\ninput file2.txt:"
hdfs dfs -cat input/file2.txt
